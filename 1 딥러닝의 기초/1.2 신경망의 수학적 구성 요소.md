## 2 신경망의 수학적 구성 요소

----

용어정리
* 범주 category : 클래스 class
* 데이터 포인트 data point : 샘플 sample
* 특정 샘플의 클래스 : 레이블 label

* 넘파이 Numpy : 파이썬 대표 다차원 배열 라이브러리
* 훈련 세트 training set : 훈련 용도 데이터셋
* 테스트 세트 test set : 테스트 용도 데이터셋

* 층 layer : 신경망의 핵심 구성 요소, 일종의 데이터 처리 필터
* 손실 함수 loss function : 훈련 데이터에서 신경망의 성능을 측정하는 방법, 네트워크가 옳은 방향으로 학습될 수 있도록 도움
* 옵티마이저 optimizer : 입력된 데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 메커니즘

* 과대적합 overfitting: 머신 러닝 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 경향

* 텐서 tensor : 다차원 넘파이 배열, 데이터를 위한 컨테이너
* 축 axis : 차원 dimension
* 랭크 rank : 텐서의 축 개수

----

### 신경망 구조

network = models.Sequential()
network = add(layers.Dense(층, activation=활성화함수 종류, input_shape(배열크기 * 배열크기,)))

### 컴파일 단계

network.compile(optimizer="옵티마이저 종류", loss=손실함수, metrics=[모니터링지표])


----

### 신경망을 위한 데이터 표현

* 스칼라 (0D 텐서, 0차원 텐서)
: 하나의 숫자만 담고 있는 텐서

import numpy as np
x = np.array(12)
x.ndim
	* (ndim: 배열의 차원을 계산하는 함수)

* 벡터 (1D 텐서)
: 하나의 축을 가지는 텐서

x = np.array([12, 3, 4, 5, 6, 7])

* 행렬 (2D 텐서)
: 두개의 축을 가지는 텐서, 행 X 열

x = np.array([[1, 2, 3], [4, 5, 6]])

* 3D와 고차원 텐서
: 직육면체 형태의 텐서, 딥러닝에서는 보통 0~4까지의 텐서 다룸

x = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

* 텐서의 핵심 속성
	* 축의 개수(랭크) : 넘파이 라이브러리에서 ndim 속성에 저장
	* 크기 shape : 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 파이썬의 튜플
	* 데이터 타입 : 넘파이의 dtype에 저장 
		ex) float32, uint8, float64, char

----

### 넘파이로 텐서 조작하기

* 슬라이싱 slicing
: 배열에 있는 특정 원소를 선택하는 것

* 배치 데이터 batch data
: 딥러닝에서 전체 데이터셋을 작은 배치로 나누어 처리하는 데이터셋
	* 배치 축 batch axis, 배치 차원 batch dimension
	: 배치 데이터를 다룰 떄 첫 번째 축
	* 샘플 축 sample axis
	: 모든 데이터 텐서의 첫 번째 축

* 텐서 사례
	* 벡터 데이터 : 2D
		* 샘플 축 : 첫 번째 축
		* 특성 축 : 두 번째 축
	* 시계열 데이터(시퀀스 데이터) : 3D
	- 데이터에서 <u>시간, 연속된 순서</u>가 중요할 때 시간 축 포함하여 3D 텐서로 저장
		* 시간 축 : 두 번째 축 (인덱스 1)
	* 이미지 : 4D
		* 구글 텐서플로 : 채널 마지막 (samples, height, width, color_depth)
		* 씨아노 : 채널 우선 (samples, color_depth, height, width)
	* 동영상 : 5D
	- (samples, frames, height, width, color_depth)

----

### 텐서 연산 tensor operation

* 원소별 연산 element-wise operation
	* relu 함수
	* 덧셈

* 브로드캐스팅 broadcasting
	* step1 큰 텐서의 ndim에 맞추도록 작은 텐서에 축이 추가
	* step2 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복

* 텐서 점곱 tensor dot product
	* 입력 텐서의 원소들을 결합

* 텐서 크기 변환 tensor reshaping
	* 전치 transformation : 행렬의 행과 열을 바꾸는 것

* 딥러닝의 기하학적 해석
	* 복잡한 데이터의 매니폴드(manifold) 펼치기
	- 심층 네트워크의 각 층은 데이터를 조금씩 풀어 주는 변환 적용
	- 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 분해
	- ex) 종이공을 펼치는 것

----

### 신경망의 엔진 : 그래디언트 기반 최적화

신경망에 사용되는 모든 연산이 미분 가능 >> 네트워크 가중치에 대한 손실의 그래디언트 계산

* 그래디언트 : 텐서 연산의 변화율
	* 그래디언트의 반대 방향으로 행렬을 움직이면 손실 함수의 값을 줄일 수 있음

* 확률적 경사 하강법
	* 함수의 최솟값을 해석적으로 줄이는 방법
	* 가장 작은 손실 함수의 값을 만드는 가중치의 조합을 해석적으로 찾는 방법
	* gradient(f)(W) = 0
	* 그래디언트의 반대 방향으로 가중치 업데이트 하면 손실이 조금씩 감소
	* 미니 배치 확률적 경사 하강법(미니 배치 SGD) mini-batch stochastic gradient descent
	* 진정한(true) SGD : 하나의 샘플과 하나의 타깃 선택

* 역전파 알고리즘 : 변화율 연결
	* 연쇄 법칙을 신경망의 그래디언트 계산에 적용
	* 최종 손실 값에서부터 시작해 최상위 층에서 하위층으로 거꾸로 진행











